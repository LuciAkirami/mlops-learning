{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When came with a new model. We want to ask some questions. Like what has changed from previous version of model to new version. Is there any preprocessing needed? What are extra libraries that we need to run a new model\n",
    "\n",
    "And what if when running this new model in production we face some issues and roll back to old model. We need to know where the old model is stored\n",
    "\n",
    "When doing an ML task, we use the MLFlow Tracking Server to log the parameters, metrics, artifactions and also many different model versions\n",
    "\n",
    "Once we believe those models are fit for production, then we will \"register model\" to the MLFlow registry\n",
    "\n",
    "MLFlow registry is the place where we store the production ready models. So whenver a deployment engineer wants to update the models, they can take a look at the Model Registry to find the new prod ready models\n",
    "\n",
    "The MLflow Model Registry component is a centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of an MLflow Model. It provides model lineage (which MLflow experiment and run produced the model), model versioning, model aliasing, model tagging, and annotations.\n",
    "\n",
    "Model Registry does not deploy the models, instead it stores the models that are prod ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/31 14:42:21 INFO mlflow.tracking.fluent: Experiment with name 'taxi-model-registry' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/478746432289998830', creation_time=1725095541014, experiment_id='478746432289998830', last_update_time=1725095541014, lifecycle_stage='active', name='taxi-model-registry', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "# Create a new MLflow Experiment - Inside an experiment, there will be Runs\n",
    "mlflow.set_experiment(\"taxi-model-registry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to read the data, preprocess it and return it\n",
    "def read_and_preprocess(filename):\n",
    "    data = pd.read_parquet(filename)\n",
    "    \n",
    "    # create the target variable\n",
    "    data['ride_duration'] = data['tpep_dropoff_datetime'] - data['tpep_pickup_datetime'] \n",
    "    data['ride_duration'] = data['ride_duration'].apply(lambda x: x.total_seconds()/60) \n",
    "\n",
    "    # take only the data below 1 hour\n",
    "    data = data[(data['ride_duration'] >= 1) & (data['ride_duration'] <= 60)]\n",
    "\n",
    "    # # sample the data to 70k rows\n",
    "    # if len(data) > 70000:\n",
    "    #     sampled_data = data.iloc[:70000,:].copy()\n",
    "    # else:\n",
    "    #     sampled_data = data.copy()\n",
    "    sampled_data = data.copy()\n",
    "    \n",
    "    # chosing categorical\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "\n",
    "    # convert these numerical categorical features to string categorical features\n",
    "    sampled_data[categorical] = sampled_data[categorical].astype(str)\n",
    "\n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_and_preprocess('../01-intro/data/yellow_tripdata_2021-01.parquet')\n",
    "df_valid = read_and_preprocess('../01-intro/data/yellow_tripdata_2021-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosing categorical and numerical features\n",
    "categorical = ['PULocationID', 'DOLocationID']\n",
    "numerical = ['trip_distance']\n",
    "\n",
    "# to use the DictVectorizer, we need to convert the dataframe to dict\n",
    "train_dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "val_dicts = df_valid[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "\n",
    "dv = DictVectorizer()\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "X_valid = dv.fit_transform(val_dicts)\n",
    "\n",
    "# storing our target variable\n",
    "target = 'ride_duration'\n",
    "y_train = df_train[target].values\n",
    "y_val = df_valid[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/31 15:08:11 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2024/08/31 15:08:11 INFO mlflow.tracking.fluent: Autologging successfully enabled for lightgbm.\n",
      "2024/08/31 15:08:11 INFO mlflow.tracking.fluent: Autologging successfully enabled for xgboost.\n",
      "2024/08/31 15:08:14 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'numpy.ndarray' object has no attribute 'toarray'\n",
      "2024/08/31 15:08:21 INFO mlflow.tracking._tracking_service.client: üèÉ View run agreeable-skink-171 at: http://127.0.0.1:5000/#/experiments/478746432289998830/runs/47ec2cd3efd744f398c9381cfab20f6d.\n",
      "2024/08/31 15:08:21 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/478746432289998830.\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression Model\n",
    "mlflow.autolog()\n",
    "\n",
    "# as we are using Auto Log, we do not need any \"with context manager\" but if we dont use context manager, we need to specify mflow.end_run() after each run\n",
    "# here this cell is a single run, so at end of the end, we need to specifu mlflow.end_run() if not using context manager\n",
    "with mlflow.start_run():\n",
    "    # train a LinearRegression Model\n",
    "    lr = LinearRegression()\n",
    "\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions on test_data\n",
    "    y_pred = lr.predict(X_valid)\n",
    "\n",
    "    # calculate the metrics\n",
    "    rmse = root_mean_squared_error(y_val, y_pred) # squared set to False implies we are using RMSE instead MSE\n",
    "\n",
    "    # logging test metric\n",
    "    mlflow.log_metric('test_root_mean_squared_error', rmse)\n",
    "\n",
    "    # logging model name - Logging it as Param, so I can see a graph of models vs RMSE\n",
    "    mlflow.log_param('model','Linear Regression')\n",
    "\n",
    "# if not using with context manager, uncomment\n",
    "# mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/31 15:08:30 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'numpy.ndarray' object has no attribute 'toarray'\n",
      "2024/08/31 15:08:37 INFO mlflow.tracking._tracking_service.client: üèÉ View run loud-lark-824 at: http://127.0.0.1:5000/#/experiments/478746432289998830/runs/4aa4fd9eb7454abda7b05bacc5707262.\n",
      "2024/08/31 15:08:37 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/478746432289998830.\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression Model with LASSO Regularization\n",
    "with mlflow.start_run():\n",
    "    lr = Lasso()\n",
    "\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions on test_data\n",
    "    y_pred = lr.predict(X_valid)\n",
    "\n",
    "    # calculate the metrics\n",
    "    rmse = root_mean_squared_error(y_val, y_pred) # squared set to False implies we are using RMSE instead MSE\n",
    "\n",
    "    # logging test metric\n",
    "    mlflow.log_metric('test_root_mean_squared_error', rmse)\n",
    "\n",
    "    # logging model name - Logging it as Param, so I can see a graph of models vs RMSE\n",
    "    mlflow.log_param('model','LASSO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/31 15:08:46 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'numpy.ndarray' object has no attribute 'toarray'\n",
      "2024/08/31 15:08:53 INFO mlflow.tracking._tracking_service.client: üèÉ View run overjoyed-hare-3 at: http://127.0.0.1:5000/#/experiments/478746432289998830/runs/6abd28401d724ef8be3282d8942d8a5a.\n",
      "2024/08/31 15:08:53 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/478746432289998830.\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression Model with Ridge Regularization\n",
    "with mlflow.start_run():\n",
    "    # train a LinearRegression Model\n",
    "    lr = Ridge()\n",
    "\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions on test_data\n",
    "    y_pred = lr.predict(X_valid)\n",
    "\n",
    "    # calculate the metrics\n",
    "    root_mean_squared_error(y_val, y_pred)\n",
    "\n",
    "    # calculate the metrics\n",
    "    rmse = root_mean_squared_error(y_val, y_pred) # squared set to False implies we are using RMSE instead MSE\n",
    "\n",
    "    # logging test metric\n",
    "    mlflow.log_metric('test_root_mean_squared_error', rmse)\n",
    "\n",
    "    # logging model name - Logging it as Param, so I can see a graph of models vs RMSE\n",
    "    mlflow.log_param('model','Ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Regressor\n",
    "\n",
    "- n_estimators: The number of trees in the ensemble, often increased until no further improvements are seen.\n",
    "- max_depth: The maximum depth of each tree, often values are between 1 and 10.\n",
    "- eta: The learning rate used to weight each model, often set to small values such as 0.3, 0.1, 0.01, or smaller.\n",
    "- subsample: The number of samples (rows) used in each tree, set to a value between 0 and 1, often 1.0 to use all samples.\n",
    "- colsample_bytree: Number of features (columns) used in each tree, set to a value between 0 and 1, often 1.0 to use all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:14:43] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "2024/08/31 15:14:55 INFO mlflow.tracking._tracking_service.client: üèÉ View run fearless-fawn-503 at: http://127.0.0.1:5000/#/experiments/478746432289998830/runs/b838e9d27ae04a2f9fc8b8b34c767d57.\n",
      "2024/08/31 15:14:55 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/478746432289998830.\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Regressor\n",
    "with mlflow.start_run():\n",
    "    boost = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n",
    "\n",
    "    boost.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions on test_data\n",
    "    y_pred = boost.predict(X_valid)\n",
    "\n",
    "    # calculate the metrics\n",
    "    root_mean_squared_error(y_val, y_pred)\n",
    "\n",
    "    # calculate the metrics\n",
    "    rmse = root_mean_squared_error(y_val, y_pred) # squared set to False implies we are using RMSE instead MSE\n",
    "\n",
    "    # logging test metric\n",
    "    mlflow.log_metric('test_root_mean_squared_error', rmse)\n",
    "\n",
    "    # logging model name - Logging it as Param, so I can see a graph of models vs RMSE\n",
    "    mlflow.log_param('model','XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Run: data=<RunData: metrics={'test_root_mean_squared_error': 4.517709979174692}, params={'base_score': 'None',\n",
      " 'booster': 'None',\n",
      " 'colsample_bylevel': 'None',\n",
      " 'colsample_bynode': 'None',\n",
      " 'colsample_bytree': '0.8',\n",
      " 'custom_metric': 'None',\n",
      " 'device': 'None',\n",
      " 'early_stopping_rounds': 'None',\n",
      " 'eta': '0.1',\n",
      " 'eval_metric': 'None',\n",
      " 'gamma': 'None',\n",
      " 'grow_policy': 'None',\n",
      " 'interaction_constraints': 'None',\n",
      " 'learning_rate': 'None',\n",
      " 'max_bin': 'None',\n",
      " 'max_cat_threshold': 'None',\n",
      " 'max_cat_to_onehot': 'None',\n",
      " 'max_delta_step': 'None',\n",
      " 'max_depth': '7',\n",
      " 'max_leaves': 'None',\n",
      " 'maximize': 'None',\n",
      " 'min_child_weight': 'None',\n",
      " 'model': 'Linear Regression',\n",
      " 'monotone_constraints': 'None',\n",
      " 'multi_strategy': 'None',\n",
      " 'n_jobs': 'None',\n",
      " 'num_boost_round': '1000',\n",
      " 'num_parallel_tree': 'None',\n",
      " 'objective': 'reg:squarederror',\n",
      " 'random_state': 'None',\n",
      " 'reg_alpha': 'None',\n",
      " 'reg_lambda': 'None',\n",
      " 'sampling_method': 'None',\n",
      " 'scale_pos_weight': 'None',\n",
      " 'subsample': '0.7',\n",
      " 'tree_method': 'None',\n",
      " 'validate_parameters': 'None',\n",
      " 'verbose_eval': 'True',\n",
      " 'verbosity': 'None'}, tags={'mlflow.log-model.history': '[{\"run_id\": \"f16c74caba6a4c6d91051499e17f14a6\", '\n",
      "                             '\"artifact_path\": \"model\", \"utc_time_created\": '\n",
      "                             '\"2024-08-31 09:39:35.987789\", \"flavors\": '\n",
      "                             '{\"python_function\": {\"loader_module\": '\n",
      "                             '\"mlflow.xgboost\", \"python_version\": \"3.11.5\", '\n",
      "                             '\"data\": \"model.xgb\", \"env\": {\"conda\": '\n",
      "                             '\"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, '\n",
      "                             '\"xgboost\": {\"xgb_version\": \"2.1.1\", \"data\": '\n",
      "                             '\"model.xgb\", \"model_class\": '\n",
      "                             '\"xgboost.sklearn.XGBRegressor\", \"model_format\": '\n",
      "                             '\"xgb\", \"code\": null}}, \"model_uuid\": '\n",
      "                             '\"8cd3bece719141c1adf3a7aaf00d80bd\"}]',\n",
      " 'mlflow.runName': 'likeable-turtle-73',\n",
      " 'mlflow.source.name': '/home/topisano/Desktop/projects/mlops-learning/venv/lib/python3.11/site-packages/ipykernel_launcher.py',\n",
      " 'mlflow.source.type': 'LOCAL',\n",
      " 'mlflow.user': 'topisano'}>, info=<RunInfo: artifact_uri='mlflow-artifacts:/478746432289998830/f16c74caba6a4c6d91051499e17f14a6/artifacts', end_time=1725097187818, experiment_id='478746432289998830', lifecycle_stage='active', run_id='f16c74caba6a4c6d91051499e17f14a6', run_name='likeable-turtle-73', run_uuid='f16c74caba6a4c6d91051499e17f14a6', start_time=1725097146761, status='FINISHED', user_id='topisano'>, inputs=<RunInputs: dataset_inputs=[<DatasetInput: dataset=<Dataset: digest='73555dd4', name='dataset', profile=('{\"features_shape\": [1340859, 519], \"features_size\": 695905821, '\n",
      " '\"features_nbytes\": 5567246568}'), schema=('{\"mlflow_tensorspec\": {\"features\": \"[{\\\\\"type\\\\\": \\\\\"tensor\\\\\", '\n",
      " '\\\\\"tensor-spec\\\\\": {\\\\\"dtype\\\\\": \\\\\"float64\\\\\", \\\\\"shape\\\\\": [-1, 519]}}]\", '\n",
      " '\"targets\": null}}'), source=('{\"tags\": {\"mlflow.user\": \"topisano\", \"mlflow.source.name\": '\n",
      " '\"/home/topisano/Desktop/projects/mlops-learning/venv/lib/python3.11/site-packages/ipykernel_launcher.py\", '\n",
      " '\"mlflow.source.type\": \"LOCAL\"}}'), source_type='code'>, tags=[<InputTag: key='mlflow.data.context', value='eval'>]>,\n",
      " <DatasetInput: dataset=<Dataset: digest='8748cccf', name='dataset', profile=('{\"features_shape\": [1343254, 519], \"features_size\": 697148826, '\n",
      " '\"features_nbytes\": 2788595304}'), schema=('{\"mlflow_tensorspec\": {\"features\": \"[{\\\\\"type\\\\\": \\\\\"tensor\\\\\", '\n",
      " '\\\\\"tensor-spec\\\\\": {\\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [-1, 519]}}]\", '\n",
      " '\"targets\": null}}'), source=('{\"tags\": {\"mlflow.user\": \"topisano\", \"mlflow.source.name\": '\n",
      " '\"/home/topisano/Desktop/projects/mlops-learning/venv/lib/python3.11/site-packages/ipykernel_launcher.py\", '\n",
      " '\"mlflow.source.type\": \"LOCAL\"}}'), source_type='code'>, tags=[<InputTag: key='mlflow.data.context', value='train'>]>]>>\n"
     ]
    }
   ],
   "source": [
    "# to get the information on last active run\n",
    "autolog_run = mlflow.last_active_run()\n",
    "print(autolog_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1343254, 519)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1245\n",
      "[LightGBM] [Info] Number of data points in the train set: 1343254, number of used features: 496\n",
      "[LightGBM] [Info] Start training from score 11.644064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/31 15:20:23 INFO mlflow.tracking._tracking_service.client: üèÉ View run intelligent-pig-197 at: http://127.0.0.1:5000/#/experiments/478746432289998830/runs/d1cc89c1a3064fb690b2e76058f06d57.\n",
      "2024/08/31 15:20:23 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/478746432289998830.\n"
     ]
    }
   ],
   "source": [
    "# LightGBM Regressor\n",
    "with mlflow.start_run():\n",
    "    boost = LGBMRegressor()\n",
    "\n",
    "    boost.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions on test_data\n",
    "    y_pred = boost.predict(X_valid)\n",
    "\n",
    "    # calculate the metrics\n",
    "    root_mean_squared_error(y_val, y_pred)\n",
    "\n",
    "    # calculate the metrics\n",
    "    rmse = root_mean_squared_error(y_val, y_pred) # squared set to False implies we are using RMSE instead MSE\n",
    "\n",
    "    # logging test metric\n",
    "    mlflow.log_metric('test_root_mean_squared_error', rmse)\n",
    "\n",
    "    # logging model name - Logging it as Param, so I can see a graph of models vs RMSE\n",
    "    mlflow.log_param('model','LGBMRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_auto_logged_info(run):\n",
    "    tags = {k: v for k, v in run.data.tags.items() if not k.startswith(\"mlflow.\")}\n",
    "    artifacts = [\n",
    "        f.path for f in mlflow.MlflowClient().list_artifacts(run.info.run_id, \"model\")\n",
    "    ]\n",
    "    feature_importances = [\n",
    "        f.path\n",
    "        for f in mlflow.MlflowClient().list_artifacts(run.info.run_id)\n",
    "        if f.path != \"model\"\n",
    "    ]\n",
    "    print(f\"run_id: {run.info.run_id}\")\n",
    "    print(f\"artifacts: {artifacts}\")\n",
    "    print(f\"feature_importances: {feature_importances}\")\n",
    "    print(f\"params: {run.data.params}\")\n",
    "    print(f\"metrics: {run.data.metrics}\")\n",
    "    print(f\"tags: {tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: d1cc89c1a3064fb690b2e76058f06d57\n",
      "artifacts: ['model/MLmodel', 'model/conda.yaml', 'model/model.pkl', 'model/python_env.yaml', 'model/requirements.txt']\n",
      "feature_importances: ['feature_importance_gain.json', 'feature_importance_gain.png', 'feature_importance_split.json', 'feature_importance_split.png']\n",
      "params: {'num_boost_round': '100', 'reg_lambda': '0.0', 'objective': 'regression', 'min_split_gain': '0.0', 'subsample_freq': '0', 'subsample': '1.0', 'num_leaves': '31', 'model': 'LGBMRegressor', 'categorical_feature': 'auto', 'learning_rate': '0.1', 'min_child_samples': '20', 'feature_name': 'auto', 'random_state': 'None', 'min_child_weight': '0.001', 'metric': \"['regression']\", 'keep_training_booster': 'False', 'num_threads': '8', 'reg_alpha': '0.0', 'max_depth': '-1', 'colsample_bytree': '1.0', 'boosting_type': 'gbdt', 'subsample_for_bin': '200000'}\n",
      "metrics: {'test_root_mean_squared_error': 4.555352288314773}\n",
      "tags: {}\n"
     ]
    }
   ],
   "source": [
    "# fetch the auto logged parameters and metrics\n",
    "autolog_run = mlflow.last_active_run()\n",
    "print_auto_logged_info(mlflow.get_run(run_id=autolog_run.info.run_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
